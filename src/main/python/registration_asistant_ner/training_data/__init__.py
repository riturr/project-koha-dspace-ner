import os
from pathlib import Path
import random

from pandas import DataFrame, Series
from spacy.tokens import DocBin
from spacy.tokens.doc import Doc

from registration_asistant_ner.training_data.data_loader import load_scraped_data
from registration_asistant_ner.training_data.data_preparer import prepare_data, generate_doc_with_entities, \
    generate_training_data
from datetime import datetime
import pandas as pd

import logging

logging.basicConfig(
    format='%(asctime)s %(levelname)-8s %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

VALID_ENTITIES = ['authors', 'advisors', 'year', 'faculty', 'program', 'title']


def generate_training_data_files(
        index_file: Path,
        files_path: Path,
        from_columns: list[str],
        training_files_path: Path = Path(os.getcwd()),
        training_files_prefix: str = datetime.today().strftime('%Y%m%d'),
        **kwargs
):
    """
    Get the training data from the XML files and PDF files.
    """
    logger.info("Loading the scraped data.")
    if os.path.exists(training_files_path / "loaded_data.pkl"):
        raw_data: DataFrame = pd.read_pickle(training_files_path / "loaded_data.pkl")
        logger.info("File loaded_data.pkl found. Loading the data from the file.")
    else:
        raw_data: DataFrame = load_scraped_data(index_file, files_path, **kwargs)
        raw_data.to_pickle(training_files_path / "loaded_data.pkl")

    logger.info("Preparing the data.")
    prepared_data: DataFrame = prepare_data(raw_data)

    logger.info("Generating the training data.")
    training_data: Series = generate_training_data(data=prepared_data, from_columns=from_columns)
    training_data_as_list: list[Doc] = training_data.to_list()

    # shuffle the data
    random.shuffle(training_data_as_list)

    # Split the data into training and test data (80% training, 20% test)
    train_data = DocBin(docs=training_data_as_list[0:int(len(training_data_as_list) * 0.8)])
    test_data = DocBin(docs=training_data_as_list[int(len(training_data_as_list) * 0.8):])

    logger.info("Saving the training data to disk.")
    train_data.to_disk(training_files_path / f"{training_files_prefix}_training.spacy")
    test_data.to_disk(training_files_path / f"{training_files_prefix}_test.spacy")


if __name__ == '__main__':
    logger.info("Starting the training data generation process.")

    import argparse

    parser = argparse.ArgumentParser(description="Get training data from XML and PDF files.")
    parser.add_argument('index_file', type=Path, help="Path to the index file generated by the scraper.")
    parser.add_argument('files_path', type=Path, help="Path to the files directory generated by the scraper.")
    parser.add_argument('--entities', type=str, nargs='+', default=['authors', 'advisors'],
                        help=f"Type of entities to extract. Valid values are {VALID_ENTITIES}.")
    parser.add_argument('--training_files_path', type=Path, default=Path(os.getcwd()),
                        help="Path to save the training files.")
    parser.add_argument('--training_files_prefix', type=str, default=datetime.today().strftime('%Y%m%d'),
                        help="Prefix for the training files. Default is the current date in the format 'YYYYMMDD'.")

    args = parser.parse_args()

    if not all(entity in VALID_ENTITIES for entity in args.entities):
        raise ValueError(f"Invalid entity. Valid values are {VALID_ENTITIES}.")

    generate_training_data_files(
        index_file=Path(args.index_file),
        files_path=Path(args.files_path),
        from_columns=args.entities,
        training_files_path=args.training_files_path,
        training_files_prefix=args.training_files_prefix
    )

    logger.info("Training data generation process completed.")
